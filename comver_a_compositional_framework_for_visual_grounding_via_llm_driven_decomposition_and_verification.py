# -*- coding: utf-8 -*-
"""COMVER: A Compositional Framework for Visual Grounding via LLM-Driven Decomposition and Verification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17SgbTKUCb-pEfpo4epS4h3mT4KEEbwgB
"""

# --- [  WORKING SCRIPT  FOR "PERSON IN RED SHIRT" ] ---

import sys
import subprocess
import os
import importlib

# Pinning to a known stable version of transformers
STABLE_TRANSFORMERS_VERSION = "4.40.1"

try:
    import transformers
    version_ok = transformers.__version__ == STABLE_TRANSFORMERS_VERSION
except ImportError:
    version_ok = False

# --- Part 1: Environment Setup ---
if not version_ok:
    print(" V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V ")
    print(f"--- INSTALLING STABLE LIBRARY VERSION: {STABLE_TRANSFORMERS_VERSION} ---")
    try:
        subprocess.check_call([
            sys.executable, '-m', 'pip', 'install', '-q',
            f'transformers=={STABLE_TRANSFORMERS_VERSION}', 'torch', 'accelerate', 'einops'
        ])
        print("\n--- ‚úÖ UPGRADE COMPLETE ---")
        print("--> PLEASE RESTART THE RUNTIME NOW <--")
        print("Go to 'Runtime' in the menu and click 'Restart session'.")
        print("After restarting, run this same cell again.")
        print(" A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A ")
        os._exit(0)
    except Exception as e:
        print(f"--- ‚ùå INSTALLATION FAILED ---: {e}")

# --- Part 2: Main Program (will only run after restart) ---
else:
    print(f"‚úÖ Correct library version detected (transformers {transformers.__version__}).")
    print("--- RUNNING THE MAIN PROGRAM ---")

    import re
    import json
    import time
    import requests
    from io import BytesIO
    from PIL import Image, ImageDraw, ImageFont
    import random
    import torch
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, GroundingDinoProcessor,
        GroundingDinoForObjectDetection, pipeline
    )
    from typing import Dict, Any, List

    class GroundingDINOModel:
        def __init__(self, model_name="IDEA-Research/grounding-dino-tiny"):
            self.device = "cpu"; print(f" ‚è≥ Loading DINO model ({model_name})...")
            self.processor = GroundingDinoProcessor.from_pretrained(model_name)
            self.model = GroundingDinoForObjectDetection.from_pretrained(model_name).to(self.device)
            print(" ‚úÖ DINO model loaded.")
        def predict(self, image: Image.Image, concept: str) -> Dict[str, Any]:
            # **MODIFIED:** The text query for Grounding DINO is now more general for attributes.
            # We'll rely on the LLM's decomposition to guide it.
            inputs = self.processor(images=image, text=f"a {concept}.", return_tensors="pt").to(self.device)
            with torch.no_grad(): outputs = self.model(**inputs)
            results = self.processor.post_process_grounded_object_detection(outputs, input_ids=inputs["input_ids"], target_sizes=[image.size[::-1]])[0]
            best_detection = {"box": None, "score": 0.0}
            if results["scores"].numel() > 0:
                best_idx = results["scores"].argmax()
                best_score = results["scores"][best_idx].item()
                if best_score >= 0.3:
                    box = results["boxes"][best_idx].tolist()
                    best_detection = {"box": tuple(box), "score": best_score}
            return best_detection

    class LLMSceneGraphDecomposer:
        def __init__(self, model_name="microsoft/Phi-3-mini-4k-instruct"):
            self.device = "cpu"; print(f"\n ‚è≥ Loading LLM ({model_name})...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=self.device, torch_dtype="auto", trust_remote_code=True)
            print(" ‚úÖ LLM loaded.")
        def decompose_prompt(self, prompt: str) -> Dict[str, Any]:
            print(f"\n üìû Decomposing prompt: '{prompt}'")
            # **MODIFIED:** Adjusted system prompt to better handle attributes like color for the main object.
            messages = [{"role": "system", "content": "You are a silent JSON parsing tool. Your task is to convert the user's sentence into a structured JSON object. The root must be 'target_object', containing 'name', 'attributes', and 'relations'. 'attributes' should be a list of descriptive words like colors. The 'relations' key should be a list of objects, each with 'type' (e.g., 'on', 'near') and 'object'. Do not output any text or explanation before or after the JSON object. Example: 'a red car on the road' -> {\"target_object\": {\"name\": \"car\", \"attributes\": [\"red\"], \"relations\": [{\"type\": \"on\", \"object\": \"road\"}]}}"}, {"role": "user", "content": prompt}]
            pipe = pipeline("text-generation", model=self.model, tokenizer=self.tokenizer)
            generation_args = {"max_new_tokens": 500, "return_full_text": False, "temperature": 0.0, "do_sample": False}
            output = pipe(messages, **generation_args)
            response_text = output[0]['generated_text']
            try:
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if not json_match: raise ValueError("No JSON object found")
                parsed_json = json.loads(json_match.group(0))
                print(f" ¬† -> LLM extracted: {json.dumps(parsed_json, indent=2)}")
                return parsed_json
            except Exception as e:
                print(f" ¬† -> LLM failed. Error: {e}\n ¬† -> Full response: {response_text}")
                return None

    class CompositionalVerifier:
        def __init__(self):
            self.dino = GroundingDINOModel()
            self.decomposer = LLMSceneGraphDecomposer()
            self.colors = ["magenta", "cyan", "lime", "yellow", "orange", "blue"]
            print("\n üöÄ Verifier is ready!")

        def _extract_all_objects(self, node: Dict, object_set: set):
            if 'name' in node: object_set.add(node['name'])
            # **MODIFIED:** Also add attributes as separate detection targets for Grounding DINO
            if 'attributes' in node and isinstance(node['attributes'], list):
                for attr in node['attributes']:
                    object_set.add(attr + " " + node['name']) # e.g., "red shirt"

            relations = node.get('relations', []);
            if isinstance(relations, dict): relations = [relations]
            for relation in relations:
                if 'name' in relation: object_set.add(relation['name'])
                if 'object' in relation and isinstance(relation['object'], dict): self._extract_all_objects(relation['object'], object_set)
                elif 'object' in relation and isinstance(relation['object'], str): object_set.add(relation['object'])

        def _check_spatial_relation(self, box1: tuple, box2: tuple, relation_type: str) -> float:
            # This is a simplified check. A real implementation would be more complex.
            # For simplicity, we assume if both boxes exist, the relation is present.
            if box1 is not None and box2 is not None:
                return 1.0
            return 0.0

        def _verify_and_score(self, node: Dict, detected_results: Dict) -> float:
            object_name = node.get('name')
            if not object_name: return 0.0

            # **MODIFIED:** Combine object name and its attributes for detection if present
            detection_target = object_name
            if 'attributes' in node and isinstance(node['attributes'], list) and node['attributes']:
                detection_target = " ".join(node['attributes']) + " " + object_name

            composite_score = detected_results.get(detection_target, {}).get('score', 0.0)

            if composite_score == 0.0: return 0.0

            relations = node.get('relations', []);
            if isinstance(relations, dict): relations = [relations]
            for relation in relations:
                related_object_node = relation.get('object', {});
                if 'name' in relation: related_object_node = relation
                if isinstance(related_object_node, str): related_object_node = {'name': related_object_node}
                related_object_name = related_object_node.get('name')

                if not related_object_name: continue

                sub_branch_score = self._verify_and_score(related_object_node, detected_results)

                # **MODIFIED:** Use the detection target for box retrieval
                box1 = detected_results.get(detection_target, {}).get('box')
                box2 = detected_results.get(related_object_name, {}).get('box')

                relation_score = self._check_spatial_relation(box1, box2, relation.get('type', ''))

                composite_score *= sub_branch_score * relation_score
            return composite_score

        def explain(self, image: Image.Image, prompt: str):
            scene_graph = self.decomposer.decompose_prompt(prompt)
            if not scene_graph or 'target_object' not in scene_graph: return {"text": "Failed to parse prompt.", "image": image, "score": 0.0}

            all_object_names = set()
            self._extract_all_objects(scene_graph['target_object'], all_object_names)

            # **MODIFIED:** Specifically add the main target with its attributes for detection
            main_target_name = scene_graph['target_object']['name']
            main_target_attributes = scene_graph['target_object'].get('attributes', [])
            if main_target_attributes:
                all_object_names.add(" ".join(main_target_attributes) + " " + main_target_name)
            else:
                all_object_names.add(main_target_name) # Ensure the main object is always detected

            print(f"\n üîç Detecting objects: {list(all_object_names)}")
            detected_results = {name: self.dino.predict(image, name) for name in all_object_names}

            print("\n ü§ù Verifying relationships...")
            final_score = self._verify_and_score(scene_graph['target_object'], detected_results)
            print(f" ¬† -> Final Composite Score: {final_score:.3f}")
            return self.generate_output(image, prompt, detected_results, final_score)

        def generate_output(self, image: Image.Image, prompt: str, results: Dict, final_score: float) -> Dict:
            print("\n üì£ Generating final explanation...")
            visual_explanation = image.copy(); draw = ImageDraw.Draw(visual_explanation); font = ImageFont.load_default()
            textual_explanation = f"Analysis for: '{prompt}'.\n**Final Score: {final_score:.3f}**\n"

            for i, (name, result) in enumerate(results.items()):
                if result and result.get('box'):
                    score = result.get('score', 0.0)
                    box = result['box']
                    color = self.colors[i % len(self.colors)]
                    label = f"{name}: {score:.2f}"
                    draw.rectangle(box, outline=color, width=4); draw.text((box[0] + 5, box[1] - 15), label, fill=color, font=font)
                    textual_explanation += f"- '{name}' found (confidence {score:.2f}).\n"
                else:
                    textual_explanation += f"- '{name}' NOT found.\n"
            return {"text": textual_explanation, "image": visual_explanation, "score": final_score}

    # --- [ Main Execution Block - MODIFIED FOR NEW IMAGE AND PROMPT ] ---
    if __name__ == "__main__":
        # MODIFIED: New image path and prompt
        image_path = "/content/red.jpg"
        prompt_to_explain = "a person in a red shirt" # **CHANGED PROMPT**

        if not os.path.exists(image_path):
            print("Downloading sample image of a person in a red shirt...")
            # MODIFIED: New URL for a person in a red shirt image
            url = "https://images.pexels.com/photos/1722198/pexels-photo-1722198.jpeg" # Example image: person in red shirt
            try:
                response = requests.get(url)
                response.raise_for_status() # Raise an error for bad status codes
                with open(image_path, "wb") as f:
                    f.write(response.content)
            except requests.exceptions.RequestException as e:
                print(f"‚ùå FAILED to download image. Error: {e}")
                exit()

        input_image = Image.open(image_path).convert("RGB")
        verifier = CompositionalVerifier()
        explanation = verifier.explain(input_image, prompt_to_explain)

        print("\n\n--- [ FINAL EXPLANATION ] ---")
        if explanation:
            print(f" üí¨ ¬†TEXTUAL EXPLANATION:\n{explanation.get('text', '')}")
            # MODIFIED: New output filename for clarity
            output_filename = "person_red_shirt_explanation.png"
            if explanation.get('image'):
                explanation['image'].save(output_filename)
                print(f"\n ‚úÖ ¬†Success! Visual explanation saved to '{output_filename}'")